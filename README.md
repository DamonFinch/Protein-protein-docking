# DeepRank Machinery Version 0.0

These files allows to :

   * assemble data from different sources (PDB,PSSM,Scores,....) in a comprehensible data base where each conformation has its own folder. In each folder are stored the conformation, features and targets.

   * Map several features to a grid. The type of features as well as the grid parameters can be freely chosen. New features can be mapped without havng to recompute old ones.

   * Use a 3d or 2d CNN to predict possible targets (binary class, haddock-score ...) from the data set

## Pre-requisite

The code is written in Python3. Several packages are required to run the code but most are pretty standard. Here is an non-exhaustive list of dependencies

  * [Numpy](http://www.numpy.org)

  * [Scipy](https://www.scipy.org/)

  * [PyTorch](http://pytorch.org)


The deep learning was implemented with PyTorch 2. (pytorch.org)
To install pytorch with anaconda 

```
conda install pytorch torchvision cuda80 -c soumith
```

  * [tensorboard](https://github.com/lanpa/tensorboard-pytorch)

To install pytorch-tensorboard with pip


```
pip install tensorboard-pytorch
```

This package depends on tensorflow-tensorboard. If you don't have tensorflow installed you can get it with

```
pip install tensorflow-tensorboard
```

The visuzalisation of the feature on the grid can be done quite easily with VMD

  * [VMD](http://www.ks.uiuc.edu/Research/vmd/)

We can develop other stategies using pyMol or other in the future.

## Overview 

The (manual) workflow contains three main stages 

1 Assemble the database from a collection of sources

2 Map the features of each conformation in the database on a grid

3 Create a torch dataset from the database and use a CNN to predict a pre-defined target

The code for each stage are contained in the own folder : **_assemble/ map/ learn/_**

### Download the data set

The docking bench mark 4 (BM4) is located on alcazar at 

```
BM4=/home/deep/HADDOCK-decoys/BM4_dimers
```

All the files needed in the following are there

decoys pdb : $BM4/decoys_pdbFLs

native pdb : $BM4/BM4_dimers_bound/pdbFLs_ori (or refined ...)

features   : $BM4/PSSM (only PSSM so far)

targets    : $BM4/model_qualities/XXX/water   (XXX=haddockscore, i-rmsd, Fnat, ....)

classID    : $BM4/training_set_IDS/classIDs.lst

We can later on add more features, and more targets.
The classIDs.lst contains the IDs of 228 complexes (114 natives / 114 decoys) preselected for training. The decoys were selected for their very low i-rmsd, i.e. they are very bad decoys.

Dowload the $BM4 folder (maybe zip it before as it is pretty big !)

### Assemble the database

The file assemble/assemble_data.py allows to collect data and to create a clean database. The data can contain natives pdbs, decoy pdbs, different features, different targets. In the output directory of the database, each conformation  has its own subfolder containing its pdb, features files and target data.

To test the routine, change the path to the BM4 folder in assemble.py at line 223. Then simply type

```
python assemble_data.py
```

to assemble the database. There are several example of use at the end of assemble.py to add a feature or a target to an existing dataset. For example

```python
da = DataAssembler(targets={'i-rmsd' : BM4 /path/to/irmsd},outdir='../training_set')
da.add_target()
```

will add a new target, here the i-rmsd, to all the complexes present in the database ../training_set.


### Map the feature to a grid

The file gridtool.py in map/ is the main class for the mapping of the features on the grid. 
This class has a lot of attributes and methods.
You can test the routine on a single conformation with

```
python gridtool.py
````

This will compute the feature of the complex contained in the ./test/ subfolder. Once done go to the subfolder. Cube files can be generated using

```
python generate_cube_files.py ./test/
```

Once the cube files generated it easy to visualize them with VMD and a few files that are automatically generated by generate_cube_files.py. 

```
cd ./test/

# we must copy the pdb manually for this example
cp 1CLV_1w.pdb data_viz/complex.pdb

cd ./data_viz/

# each feature has its own vmd script file
# to visuzalize the atomiv densities use:
vmd -e AtomicDensities.vmd
```

### Map the features of all the database

To map the features of all the complexes contained in the database, change the folder name in gendatatool.py to the training_set/ folder on your machine. Then type

```
python gendatatool.py
```

This will loop over all the folder and map all the features. The cuve files are not generated automatically as it will take to much time. The generate_cube_files.py is however automatically copied to the training_set/ folder. You can use it to visualize the data of a particular complex as before.

```
python generate_cube_files.py molfolder
cp moldfolder/viz_data
vmd -e AtomicDensities.vmd
```

### Deep Learning

The main file for the deeplearning phase is DeepRankConvNet.py. A lot of options are available here. You can in particular to use a 2d or 3d conv net and to do either a regression or a classification. The header of the file explains all the options. To start the learning loop simply type 

```
python DeepRankConvNet.py
````

