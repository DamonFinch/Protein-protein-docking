import os
import sys
import torch
import torch.utils.data as data_utils
import subprocess as sp
import numpy as np
import pickle 

try:
	from tqdm import tqdm
except:
	def tqdm(x):
		return x

class DeepRankDataSet(data_utils.Dataset):

	'''
	Class that generates the data needed for deeprank.

	ARGUMENTS 
		
	data_folder : string

		the path of the folder where the numpy data are stored
		for each cconformation. This folder should contains N subfolder
		each subfolder that each contains and input and output subsbufolder
		The input subsubfolder should contain a file called AtomicDensities.npy
		The output subsubfolder should contain a file called score
		This hierarchie of folder can be created with the gendatatool.py 


	filter_dataset : None, integer or file name

		Int
		Maximum number of elements required in the data set.
		It might be that the data_folder contains N conformation
		but that we only want M of them. Then we can set dataset_size = M
		In that case we select M random comformation among the N

		File Name
		the file name must contains complex ID that are in the data_folder
		the data set will be only constucted from the cmplexes specified in
		the file.

		None (default value)
		all the data are loaded

	select_feature : dict or 'all'
		
		if 'all', all the .npy files contained in the input folder 
		will be loaded

		if a dict must be of the structure
		{name : [indexes]} e.g. {AtomicDensities : [1,2]}
		or
		{name : 'all'}     e.g {PSSM : 'all'}
		the name must correspond to a .npy file generated by the grid tool
		and present in the input directory of the complex
		if the value is a list of index only thoses indexes will be loaded
		if 'all' all the data will be loaded

		Each name.npy has a shape of
		Nchannels x Nx x Ny x Nz. Where Nx Ny Nz are the number of grid points
		on each axis. Nchannels is the number of channels, i.e. the number
		of features at each grid point in this file.  


	select_target

		the name of the target we want. If target file is haddock.dat
		set select_target='haddock'

	normalize_x   :	Boolean

		normalize the values of the features or targets between 0 and 1


	USAGE

		data = DeepRankDataSet(folder_name)

	'''

	def __init__(self,data_folder,filter_dataset=None,
				 select_feature='all',select_target='binary_class',
		         normalize_features=False,normalize_targets=True):


		self.data_folder = data_folder
		self.filter_dataset = filter_dataset
		self.select_feature = select_feature
		self.select_target = select_target
		self.normalize_features = normalize_features
		self.normalize_targets = normalize_targets


		self.features = None
		self.targets  = None

		self.index_train = None
		self.index_valid = None

		self.train_sampler = None
		self.valid_sample = None

		self.input_shape = None

		# load the data
		print('\n')
		print('='*40)
		print('=\t Build data set from folder')
		print('= \t %s' %data_folder)
		print('='*40,'\n')

		#self.load_dataset()


	def __len__(self):
		return len(self.targets)

	def __getitem__(self,index):
		return self.features[index],self.targets[index]


	# Load the dataset
	def load(self):

		# read the folders name
		folders_name = sp.check_output("ls %s/*/ -d" %(self.data_folder),shell=True)
		folders_name = folders_name.decode('utf8').split()

		# get a subset
		if self.filter_dataset != None:

			# get a random subset if integers
			if isinstance(self.filter_dataset,int):
				np.random.shuffle(folders_name)
				folders_name = folders_name[:self.filter_dataset]

			# select based on name of file
			if os.path.isfile(self.filter_dataset):
				tmp_folder = []
				with open(self.filter_dataset) as f:
					for line in f:
						if len(line.split())>0:
							name = line.split()[0]
							tmp_folder += list(filter(lambda x: name in x,folders_name))
				f.close()
				folders_name = tmp_folder
				
				
		# load the data
		features, targets = [], []
		for folder in tqdm(folders_name):

			# load the all the features
			if self.select_feature == 'all' :
				feature_files  = sp.check_output('ls %s/input/*.pkl' %folder,shell=True).decode('utf-8').split()
				for f in feature_files:
					feat_dict = pickle.load(open(f,'rb'))
					for key,values in feat_dict.items():
						features.append(values)

			# load selected features only
			else:
				for feat_name,feat_channels in self.select_feature.items():

					# see if the feature exists
					try:
						feat_dict = pickle.load(open(folder+'/input/'+feat_name+'.pkl','rb'))
						possible_channels = list(feat_dict.keys())

					except:
						print('Error : Feature name %s not found in %s' %(feat_name,folder))
						opt = sp.check_output('ls '+ folder+'/input/*.pkl',shell=True).decode('utf8').split()
						opt_names = [name.split('/')[-1][:-4] for name in opt ]
						print('Error : Possible features are \n\t%s' %'\n\t'.join(opt_names))
						sys.exit()

					# make sure that all the featchanels are in the file
					if feat_channels != 'all':
						for fc in feat_channels:
							if fc not in possible_channels:
								print("Error : required key %s for feature %s not in the database" %(fc,feat_name))
								self.get_content()
								sys.exit()

					# load the feature channels
					tmp_feat = []
					for chanel_name,channel_value in feat_dict.items():
						if feat_channels == 'all' or chanel_name in feat_channels:
							tmp_feat.append(channel_value)

					# append to the list of features
					features.append(np.array(tmp_feat))

			# target
			try:
				targ_data = np.loadtxt(folder+'/targets/%s.dat' %(self.select_target))
			except:
				print('Error : Target name %s not found in %s' %(self.select_target,folder))
				opt = sp.check_output('ls '+ folder+'/targets/*.*',shell=True).decode('utf8').split()
				opt_names = [name.split('/')[-1][:-4] for name in opt ]
				print('Error : Possible targets are \n\t%s' %'\n\t'.join(opt_names))
				sys.exit()

			targets.append(targ_data)

		# get the number of channels and points along each axis
		self.input_shape = features[0].shape
		

		# transform the data in torch Tensor
		self.features = torch.FloatTensor(np.array(features))
		self.targets = torch.FloatTensor(np.array(targets))

		# normalize the targets/featurs
		if self.normalize_targets:
			self.targets -= self.targets.min()
			self.targets /= self.targets.max()

		if self.normalize_features:
			self.features -= features.min()
			self.features /= features.max()*0.5
			self.features -= 1.


	# print the content of the data in the data folder
	def get_content(self):

		print('--> Content of the database')

		# get the name of the first folder in the databse
		folder_name = sp.check_output("ls %s/*/ -d" %(self.data_folder),shell=True).decode('utf-8').split()[0]
		
		# get the pickle file names
		feature_files  = sp.check_output('ls %s/input/*.pkl' %folder_name,shell=True).decode('utf-8').split()

		# loop over all the files
		for f in feature_files:
			print('\tFile : %s' %(f.split('/')[-1]))
			feat_dict = pickle.load(open(f,'rb'))
			for key,values in feat_dict.items():
				print('\t\tkey : %s' %key)


	#convert the 3d data set to 2d data set
	def convert_dataset_to2d(self,proj2d=0):

		'''
		convert the 3D volumetric dataset to a 2D planar data set 
		to be used in 2d convolutional network
		proj2d specifies the dimension that we want to comsider as channel
		for example for proj2d = 0 the 2D images are in the yz plane and 
		the stack along the x dimension is considered as extra channels
		'''
		planes = ['yz','xz','xy']
		print(': Project 3D data set to 2D images in the %s plane ' %planes[proj2d])

		nf = self.__len__()
		nc,nx,ny,nz = self.input_shape
		if proj2d==0:
			self.features = self.features.view(nf,-1,1,ny,nz).squeeze()
		elif proj2d==1:
			self.features = self.features.view(nf,-1,nx,1,nz).squeeze()
		elif proj2d==2:
			self.features = self.features.view(nf,-1,nx,ny,1).squeeze()
		
		# get the number of channels and points along each axis
		# the input_shape is now a torch.Size object
		self.input_shape = self.features[0].shape




		