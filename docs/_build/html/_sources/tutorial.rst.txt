
Tutorial : Data Generation
===========================

This page gives a introduction of the data generation process in DeepRank. This is a breakdown of the file ``test/test_generate.py``.


The data generation process allows to compute features and targets of a collections of PDB files and store all the data in a single HDF5 file. The use of HDF5 files not only allows to save diskspace through compression but also reduce I/O time during the deep learning phase.

To have an idea on how to generate data with DeepRank you can refer to the file ``test/test_generate.py``. We are now going to go in detail through this file.

The file of course starts with

>>> from deeprank.generate import *

That imports all the submodules required to generate data.

Create the database
---------------------

The generation of the data also require a series of PDB file for which we want to compute features and targets. To specify where these files are located you can simply provide the path of the directory where these files are stored. This is called here ``pdb_source``. In the test file we set:

>>> pdb_source = ['./1AK4/decoys']

where we have stored 10 decoys of the 1AK4 complex. These 10 decoys will therefore be included in the final HDF5 file generated by DeepRank.

Most target values require the comparison of the decoy structure with those of the corresponding native conformation. Therefore we must specify where to find these native conformation. There as well you only have to specify the directory where native conformations are stored

>>> pdb_native = ['./1AK4/native']

The code will look for PDB in that folder and use the one whose name correspond to the decoy. For example if the decoy is named 1AK4_100w.pdb, the code will use the native 1AK4.pdb.

Finally we must specify the name of the HDF5 file where to store the data:

>>> hdf5 = '1ak4.hdf5'

We are now ready to initialize the ``DataGenerator`` class included in DeepRank

>>> database = DataGenerator(pdb_source=pdb_source,pdb_native=pdb_native,
>>> 	                     compute_targets  = ['deeprank.targets.dockQ'],
>>> 	                     compute_features = ['deeprank.features.AtomicFeature',
>>> 	                                         'deeprank.features.NaivePSSM',
>>> 	                                         'deeprank.features.PSSM_IC',
>>> 	                                         'deeprank.features.BSA'],
>>> 	                     hdf5=h5file)

As you can see we specify here the ``pdb_source`` and ``pdb_native`` argument of the ``DataGenerator``. We also must specify which feature and which target values must be computed for each complex. This is done by providing a list of file names that are in charge of these calculations. A detail of these files can be found here and a example on how to create and call your own feature/target module here.

The above statement initalize the calss instance but do not compute anything. To actually perform the calculation, we must use the ``create_database()`` method of the class:

>>> database.create_database(prog_bar=True)

Once you punch that in the code will fo through all the complex specified as input and compute all the feature/targets require in the definition of the class.


Adding Features/Targets
-------------------------

Suppose you've finised creating a huge database and you jusr realize you forgot to compute a specific feature or target. Do you have to recompute everything ? Well that would be silly. You can add features and targets to an existing database very simply:

>>> h5file = '1ak4.hdf5'
>>> database = DataGenerator(compute_targets  = ['deeprank.targets.binary_class'],
>>>                          compute_features  = ['deeprank.features.ResidueDensity']
>>>                          hdf5=h5file)
>>>
>>> # add targets/features
>>> database.add_target()
>>> database.add_feature()
>>>
>>> # map features
>>> database.map_features()

Voila. Here we simply sepcify the name of an existing hdf5 file containing the database and new features/targets to add to this database. The methods ``add_target`` ``add_feature'' are then simply called to add data to the file. Don't forget to map the new features afterwards.


Map the features
------------------
The next step consists in mapping the features calculated above to a grid of points centered around the molecule interface. Before mapping the feature, we must specify basic information about the grid. This can be done via a dictionnary:


>>> grid_info = {
>>> 	'number_of_points' : [30,30,30],
>>> 	'resolution' : [1.,1.,1.],
>>> 	'atomic_densities' : {'CA':3.5,'N':3.5,'O':3.5,'C':3.5},
>>> }

Here we specify that we want 30 points in each direction with distance of 1 Angs between each points. We must also here sepcify the atomic densities we want. While atomic densities are really a feature of the conformation, they do not require any precalculation unlike, the other features calculated above. Hence they can be directly specified in the ``grid_info``. We will see how to create a feature calculator that does the same job below to illustrate the creation of new features.

We are now able to map all the features we have calculated on the grid. This is simply done with the method call:

>>> database.map_features(grid_info,try_sparse=True)

By setting ``try_sparse`` to True the code will try to store the 3D maps as a built-in sparse format. This can seriously reduce the disck space required by the datase. As a result you must have now a file called ``1ak4.hdf5`` that contains all the data of the 10 decoys located in ``1AK4/decoys/``. You can easily explore this file and visualized the data using the DeepXplorer interface (https://github.com/DeepRank/DeepXplorer).





