
#!/usr/bin/env python
import sys
import os
import time
import h5py
import matplotlib.pyplot as plt
import numpy as np

#import torch
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data_utils


import torch.cuda

printif = lambda string,cond: print(string) if cond else None

class NeuralNet():

	'''
	Train a Convolutional Neural Network for DeepRank

	ARGUMENTS

	data_set

		Data set generated by the DeepRankDataSet
		data_set = DeepRankDataSet( ... )

	model

		definition of the NN to use. Must subclass nn.Module.
		See examples in model2D.py and model3d.py

	model_type : '2d' or '3d'

		Specify if we consider a 3D or a 2D convnet
		This ust matches the model used in the training
		if we specif a 2d CNN the data set is automatically covnerted
		to the correct format.

	proj2d : 0,1 or 2

		specify wich axis is conisdered as a channels during the conversion from
		3d to 2d data type.
		0 -> x-axis is a channel i.e the images are in the yz plane
		1 -> y-axis is a channel i.e the images are in the xz plane
		1 -> z-axis is a channel i.e the images are in the xy plane

	task : 'ref' or 'class'

		Task to perform either
		'reg'   for regression
		'class' for classification
		The loss function, the format of the targets and plot functions
		will be autmatically adjusted dependinf on the task

	plot : True/False

		So fat only a scatter plots of the outputs VS targets
		for training and validatiion set is exported.

	outdir

		output directory where all the files will be written

	USAGE

		net = NeuralNet( ... )
		(optional) net.optimiser = optim.Adam( ... )
		(optional) net.criterion = nn.CrossEntropy( .... )
		net.train( nepoch=50 )

	'''

	def __init__(self,data_set,model,
				 model_type='3d',proj2d=0,task='reg',
				 cuda=False,ngpu=0,
		         plot=True,outdir='./'):



		#data set and model
		self.data_set = data_set

		# convert the data to 2d if necessary
		if model_type == '2d':

			self.data_set.transform = True
			self.data_set.proj2D = proj2d

			dstype = self.data_set.__class__.__name__
			if  dstype == 'InMemoryDataSet':
				self.data_set.convert_dataset_to2d()
			elif dstype == 'DataSet':
				self.data_set.get_input_shape()
			else:
				raise ValueError('Data set type %s not recognized' %self.data_set.__name__)

		# task to accomplish
		self.task = task

		# CUDA required
		self.cuda = cuda
		self.ngpu = ngpu

		# handles GPU/CUDA
		if self.ngpu > 0:
			self.cuda = True

		if self.ngpu == 0 and self.cuda :
			self.ngpu = 1

		# plot or not plot
		self.plot = plot

		# Set the loss functiom
		if self.task=='reg':
			self.criterion = nn.MSELoss(size_average=False)
			self._plot_scatter = self._plot_scatter_reg

		elif self.task=='class':
			self.criterion = nn.CrossEntropyLoss()
			self._plot_scatter = self._plot_boxplot_class

		else:
			raise ValueError("Task " + self.task +"not recognized.\nOptions are \n\t 'reg': regression \n\t 'class': classifiation\n\n")


		# output directory
		self.outdir = outdir
		if self.plot:
			if not os.path.isdir(self.outdir):
				os.mkdir(outdir)

		print('\n')
		print('='*40)
		print('=\t Convolution Neural Network')
		print('=\t model     : %s' %model_type)
		print('=\t CNN       : %s' %model.__name__)

		for feat_type,feat_names in self.data_set.select_feature.items():
			print('=\t features  : %s' %(feat_type))
			for name in feat_names:	
				print('=\t\t     %s' %(name))
		print('=\t targets   : %s' %self.data_set.select_target)
		print('=\t CUDA      : %s' %str(self.cuda))
		if self.cuda:
			print('=\t nGPU      : %d' %self.ngpu)
		print('='*40,'\n')

		# check if CUDA works
		if self.cuda and not torch.cuda.is_available():
			print(' --> CUDA not deteceted : Make sure that CUDA is installed and that you are running on GPUs')
			print(' --> To turn CUDA of set cuda=False in NeuralNet')
			print(' --> Aborting the experiment \n\n')
			sys.exit()


		# load the model
		self.net = model(data_set.input_shape)

		#multi-gpu
		if self.ngpu>1:
			ids = [i for i in range(self.ngpu)]
			self.net = nn.DataParallel(self.net,device_ids=ids).cuda()

		# cuda compatible
		elif self.cuda:
			self.net = self.net.cuda()

		# set the optimizer
		self.optimizer = optim.SGD(self.net.parameters(),lr=0.005,momentum=0.9,weight_decay=0.001)

	def train(self,nepoch=50, percent_train=0.8, hdf5='data.hdf5',train_batch_size = 10,
		      preshuffle = True,export_intermediate=True,num_workers=1):

		'''
		Perform a simple training of the model. The data set is divided in training/validation sets

		ARGUMENTS

		nepoch : Int. number of iterations to go through the training

		divide_set : the percentage assign to the training, validation and test set.

		train_batch_size : the mini batch size for the training

		preshuffle. Boolean Shuffle the data set before dividing it.

		plot_intermediate : plot scatter plots during the training

		'''

		# multi-gpu
		if self.ngpu > 1:
			train_batch_size *= self.ngpu

		print('\n: Batch Size : %d' %train_batch_size)
		if self.cuda:
			print(': NGPU       : %d' %self.ngpu)

		# hdf5 support
		fname =self.outdir+'/'+hdf5
		if os.path.isfile(fname):
			fname = os.path.splitext(fname)[0] + '_new.hdf5'
		self.f5 = h5py.File(fname,'w')


		# divide the set in train+ valid and test
		divide_set = [percent_train,1.-percent_train]
		index_train,index_valid,index_test = self._divide_dataset(divide_set,preshuffle)

		print(': %d confs. for training' %len(index_train))
		print(': %d confs. for validation' %len(index_valid))
		print(': %d confs. for testing' %len(index_test))

		# train the model
		t0 = time.time()
		self._train(index_train,index_valid,index_test,
			        nepoch=nepoch,
			        train_batch_size=train_batch_size,
			        export_intermediate=export_intermediate,
			        num_workers=num_workers)
		self.f5.close()
		print(' --> Training done in ', time.strftime('%H:%M:%S', time.gmtime(time.time()-t0)))

	def test(self):

		index = list(range(self.data_set.__len__()))
		sampler = data_utils.sampler.SubsetRandomSampler(index)
		loader = data_utils.DataLoader(self.data_set,sampler=sampler)
		self.data = {}
		_,self.data['test'] = self._epoch(loader,train_model=False)
		self._plot_scatter_reg(self.outdir+'/test.png')

	def save_model(self,filename='model.pth.tar'):
		'''
		save the model to disk
		'''
		filename = self.outdir + '/' + filename
		state = {'state_dict'   : self.net.state_dict(),
				'optimizer'    : self.optimizer.state_dict(),
				'normalize_targets' : self.data_set.normalize_targets,
				'normalize_features': self.data_set.normalize_features,
				'transform'   : self.data_set.transform,
				'proj2D'      : self.data_set.proj2D}

		if self.data_set.normalize_features:
			state['feature_mean'] =  self.data_set.feature_mean
			state['feature_std' ] = self.data_set.feature_std

		if self.data_set.normalize_targets:
			state['target_min']  = self.data_set.target_min
			state['target_max']  = self.data_set.target_max
		torch.save(state,filename)

	def load_model(self,filename):
		'''
		load model
		'''
		state = torch.load(filename)
		self.net.load_state_dict(state['state_dict'])
		self.optimizer.load_state_dict(state['optimizer'])
		self.data_set.normalize_targets = state['normalize_targets']
		if self.data_set.normalize_targets:
			self.data_set.target_min = state['target_min']
			self.data_set.target_max = state['target_max']
		self.data_set.normalize_features = state['normalize_features']
		if self.data_set.normalize_features:
			self.data_set.feature_mean = state['feature_mean']
			self.data_set.feature_std = state['feature_std']
		self.data_set.transform = state['transform']
		self.data_set.proj2D = state['proj2D']

	def _divide_dataset(self,divide_set, preshuffle):

		'''
		Divide the data set in atraining validation and test
		according to the percentage in divide_set
		Retun the indexes of  each set
		'''

		if preshuffle:
			np.random.shuffle(self.data_set.index_train)

		# size of the subset for training
		ntrain = int( float(self.data_set.ntrain)*divide_set[0] )

		# indexes
		index_train = self.data_set.index_train[:ntrain]
		index_valid = self.data_set.index_train[ntrain:self.data_set.ntrain]
		index_test = self.data_set.index_test

		return index_train,index_valid,index_test



	def _train(self,index_train,index_valid,index_test,
		       nepoch = 50,train_batch_size = 5,
		       export_intermediate=False,num_workers=1):

		'''
		Train the model

		Arguments

		index_train : the indexes of the training set
		index_valid : the indexes of the validation set
		nepoch : number of epochs to be performed
		train_batch_size : the mini batch size for the training
		tensorboard_write : the writer for tensor board
		plot_intermediate : plot itnermediate
		debug : if TRUE the data set are statically created
		'''

		# printing options
		nprint = np.max([1,int(nepoch/10)])

		# store the length of the training set
		ntrain = len(index_train)

		# pin memory for cuda
		pin = False
		if self.cuda:
			pin = True

		# create the sampler
		train_sampler = data_utils.sampler.SubsetRandomSampler(index_train)
		valid_sampler = data_utils.sampler.SubsetRandomSampler(index_valid)
		test_sampler = data_utils.sampler.SubsetRandomSampler(index_test)

		# get if we test as well
		_test_ = len(test_sampler.indices)>0

		# containers for the losses
		self.losses={'train': [],'valid': []}
		if _test_:
			self.losses['test'] = []


		#  create the loaders
		train_loader = data_utils.DataLoader(self.data_set,batch_size=train_batch_size,sampler=train_sampler,pin_memory=pin,num_workers=num_workers,shuffle=False,drop_last=False)
		valid_loader = data_utils.DataLoader(self.data_set,batch_size=train_batch_size,sampler=valid_sampler,pin_memory=pin,num_workers=num_workers,shuffle=False,drop_last=False)

		if _test_:
			test_loader = data_utils.DataLoader(self.data_set,batch_size=train_batch_size,sampler=test_sampler,pin_memory=pin,num_workers=num_workers,shuffle=False,drop_last=False)

		# training loop
		av_time = 0.0
		self.data = {}
		for epoch in range(nepoch):

			print('\n: epoch %03d / %03d ' %(epoch,nepoch) + '-'*45)
			t0 = time.time()

			# we valid/test before training so that the data for the three
			# are calcualted with the same NN

			# validate the model
			self.valid_loss,self.data['valid'] = self._epoch(valid_loader,train_model=False)
			self.losses['valid'].append(self.valid_loss)

			# test the model
			if _test_:
				test_loss,self.data['test'] = self._epoch(test_loader,train_model=False)
				self.losses['test'].append(test_loss)

			# process train data without training
			#self.train_loss,self.data['train'] = self._epoch(train_loader,train_model=False	)
			#self.losses['train'].append(self.train_loss)

			# train the model
			self.train_loss,self.data['train'] = self._epoch(train_loader,train_model=True)
			self.losses['train'].append(self.train_loss)

			# talk a bit about losse
			print('  train loss       : %1.3e\n  valid loss       : %1.3e' %(self.train_loss, self.valid_loss))
			if _test_:
				print('  test loss        : %1.3e' %(test_loss))

			# timer
			elapsed = time.time()-t0
			if elapsed>10:
				print('  epoch done in    :', time.strftime('%H:%M:%S', time.gmtime(elapsed)))
			else:
				print('  epoch done in    : %1.3f' %elapsed)

			# remaining time
			av_time += elapsed
			nremain = nepoch-(epoch+1)
			remaining_time = av_time/(epoch+1)*nremain
			print('  remaining time   :',  time.strftime('%H:%M:%S', time.gmtime(remaining_time)))

			# plot the scatter plots
			if (export_intermediate and epoch%nprint == nprint-1) or epoch==0 or epoch==nepoch-1:
				if self.plot:
					figname = self.outdir+"/prediction_%03d.png" %epoch
					self._plot_scatter(figname)

					figname = self.outdir+"/hitrate_%03d.png" %epoch
					self.plot_hit_rate(figname)

				self._export_epoch_hdf5(epoch,self.data)

			sys.stdout.flush()

		# plot the losses
		self._export_losses(self.outdir+'/'+'losses.png')

		return torch.cat([param.data.view(-1) for param in self.net.parameters()],0)

	def _epoch(self,data_loader,train_model):

		'''
		Perform one single epoch iteration over a data loader
		The option train is True or False and controls
		if the model should be trained or not on the data
		The loss of the model is returned
		'''

		running_loss = 0
		data = {'outputs':[],'targets':[],'mol':[]}
		n = 0
		debug_time = False
		time_learn = 0

		for d in data_loader:

			# get the data
			inputs = d['feature']
			targets = d['target']
			mol = d['mol']

			# transform the data
			inputs,targets = self._get_variables(inputs,targets)

			# zero gradient
			tlearn0 = time.time()

			# forward + loss
			outputs = self.net(inputs)
			loss = self.criterion(outputs,targets)
			running_loss += loss.data[0]
			n += len(inputs)

			# zero + backward + step
			if train_model:
				self.optimizer.zero_grad()
				loss.backward()
				self.optimizer.step()
			time_learn += time.time()-tlearn0

			# get the outputs for export
			if self.cuda:
				data['outputs'] +=  outputs.data.cpu().numpy().tolist()
				data['targets'] += targets.data.cpu().numpy().tolist()
			else:
				data['outputs'] +=  outputs.data.numpy().tolist()
				data['targets'] += targets.data.numpy().tolist()

			fname,molname = mol[0],mol[1]
			data['mol'] += [ (f,m) for f,m in zip(fname,molname)]

		# transform the output back
		if self.data_set.normalize_targets:
			data['outputs']  = self.data_set.backtransform_target(np.array(data['outputs']).flatten())
			data['targets']  = self.data_set.backtransform_target(np.array(data['targets']).flatten())
		else:
			data['outputs']  = np.array(data['outputs']).flatten()
			data['targets']  = np.array(data['targets']).flatten()

		# normalize the loss
		running_loss /= n
		if train_model:
			printif('     __train__ %f' %time_learn,debug_time)

		return running_loss, data


	def _get_variables(self,inputs,targets):

		'''
		Convert the inout/target in Variables
		the format is different for regression where the targets are float
		and classification where they are int.
		'''

		# if cuda is available
		if self.cuda:
			inputs = inputs.cuda(async=True)
			targets = targets.cuda(async=True)


		# get the varialbe as float by default
		inputs,targets = Variable(inputs).float(),Variable(targets).float()

		# change the targets to long for classification
		if self.task == 'class':
			targets =  targets.long()

		return inputs,targets


	def _export_losses(self,figname):

		'''
		plot the losses vs the epoch
		'''

		print('\n --> Loss Plot')

		color_plot = ['red','blue','green']
		labels = ['Train','Valid','Test']

		fig,ax = plt.subplots()
		for ik,name in enumerate(self.losses):
			plt.plot(np.array(self.losses[name]),c=color_plot[ik],label=labels[ik])

		legend = ax.legend(loc='upper left')
		ax.set_xlabel('Epoch')
		ax.set_ylabel('Losses')

		fig.savefig(figname)
		plt.close()

		grp = self.f5.create_group('/losses/')
		grp.attrs['type'] = 'losses'
		for k,v in self.losses.items():
			grp.create_dataset(k,data=v)

	def _plot_scatter_reg(self,figname):

		'''
		Plot a scatter plots of predictions VS targets useful '
		to visualize the performance of the training algorithm
		'''

		# abort if we don't want to plot
		if self.plot is False:
			return


		print('\n --> Scatter Plot : ', figname, '\n')

		color_plot = {'train':'red','valid':'blue','test':'green'}
		labels = ['train','valid','test']

		fig,ax = plt.subplots()

		xvalues = np.array([])
		yvalues = np.array([])

		for l in labels:

			if l in self.data:

				targ = self.data[l]['targets']
				out = self.data[l]['outputs']

				xvalues = np.append(xvalues,targ)
				yvalues = np.append(yvalues,out)

				ax.scatter(targ,out,c = color_plot[l],label=l)

		legend = ax.legend(loc='upper left')
		ax.set_xlabel('Targets')
		ax.set_ylabel('Predictions')

		values = np.append(xvalues,yvalues)
		border = 0.1 * (values.max()-values.min())
		ax.plot([values.min()-border,values.max()+border],[values.min()-border,values.max()+border])

		fig.savefig(figname)
		plt.close()


	def plot_hit_rate(self,figname,irmsd_thr = 4.0,inverse=False):

		'''
		plot the hit rate of the different training/valid/test sets
		The hit rate is defined as:
		the percentage of positive decoys that are included among the top m decoys.
		a positive decoy is a native-like one with a i-rmsd < 4A
		'''

		if self.plot is False:
			return

		print('\n --> Hit Rate :', figname, '\n')

		color_plot = {'train':'red','valid':'blue','test':'green'}
		labels = ['train','valid','test']

		fig,ax = plt.subplots()

		for l in labels:

			if l in self.data:

				# get the target values
				out = self.data[l]['outputs']

				# get the irmsd
				irmsd = []
				for fname,mol in self.data[l]['mol']:

					f5 = h5py.File(fname,'r')
					irmsd.append(f5[mol+'/targets/IRMSD'].value)
					f5.close()

				# sort the data
				ind_sort = np.argsort(out)
				if not inverse:
					ind_sort = ind_sort[::-1]
				irmsd = np.array(irmsd)[ind_sort]

				# compute the hit rate
				hit = np.cumsum(irmsd<irmsd_thr)/len(irmsd)

				# plot
				plt.plot(hit,c = color_plot[l],label=l)

		legend = ax.legend(loc='upper left')
		ax.set_xlabel('Top M')
		ax.set_ylabel('Hit Rate')

		fig.savefig(figname)
		plt.close()



	def _export_epoch_hdf5(self,epoch,data):

		grp_name = 'epoch_%04d' %epoch
		grp = self.f5.create_group(grp_name)
		grp.attrs['type'] = 'epoch'
		for k,v in data.items():
			try:
				sg = grp.create_group(k)
				for kk,vv in v.items():
					sg.create_dataset(kk,data=vv)
			except TypeError:
				pass


	# def freeze_conv_layers(self):

	# 	for attr,vals in self.net.__dict__['_modules'].items:

	# 	for param in self.net.parameters():
	# 		param.require_grad = False




