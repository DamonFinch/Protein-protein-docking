
#!/usr/bin/env python

from datetime import datetime
import sys
import os
import time
import matplotlib.pyplot as plt
import numpy as np

#import torch
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data_utils

import torch.cuda

from deeprank.learn import DataSet

class ConvNet:

	'''
	Train a Convolutional Neural Network for DeepRank

	ARGUMENTS

	data_set
		
		Data set generated by the DeepRankDataSet
		data_set = DeepRankDataSet( ... )

	model

		definition of the NN to use. Must subclass nn.Module.
		See examples in model2D.py and model3d.py

	model_type : '2d' or '3d'

		Specify if we consider a 3D or a 2D convnet 
		This ust matches the model used in the training
		if we specif a 2d CNN the data set is automatically covnerted
		to the correct format.

	proj2d : 0,1 or 2

		specify wich axis is conisdered as a channels during the conversion from
		3d to 2d data type.
		0 -> x-axis is a channel i.e the images are in the yz plane
		1 -> y-axis is a channel i.e the images are in the xz plane
		1 -> z-axis is a channel i.e the images are in the xy plane
	
	task : 'ref' or 'class'

		Task to perform either
		'reg'   for regression 
		'class' for classification
		The loss function, the format of the targets and plot functions
		will be autmatically adjusted dependinf on the task

	tensorboard : 0/1

		Boolean to allow export in the tensorboard format.
		if set to true the logdir will be written in a dir called ./runs/
		type 
			tensorboard --logdir ./runs/ 
		to start tensorboard.
		Open a web browser and go to localhost:6006 to visualize the data

	plot : True/False

		So fat only a scatter plots of the outputs VS targets
		for training and validatiion set is exported.

	outdir

		output directory where all the files will be written

	USAGE 

		net = DeepRankConvNet( ... )
		(optional) net.optimiser = optim.Adam( ... )
		(optional) net.criterion = nn.CrossEntropy( .... )
		net.train( nepoch=50 )

	'''

	def __init__(self,data_set,model,model_type='3d',proj2d=0,
		         task='reg',cuda=False,ngpu=0,tensorboard=False,plot=True,outdir='./'):



		#data set and model
		self.data_set = data_set

		# convert the data to 2d if necessary
		if model_type == '2d':
			data_set.convert_dataset_to2d(proj2d=proj2d)

		# task to accomplish 
		self.task = task

		# CUDA required
		self.cuda = cuda
		self.ngpu = ngpu

		# handles GPU/CUDA
		if self.ngpu > 0:
			self.cuda = True

		if self.ngpu == 0 and self.cuda :
			self.ngpu = 1

		# plot or not plot
		self.plot = plot

		# import matplotlib only if we 
		if self.plot:
			import matplotlib.pyplot as plt


		# Set the loss functiom
		if self.task=='reg':
			self.criterion = nn.MSELoss()
			self._plot_scatter = self._plot_scatter_reg

		elif self.task=='class':
			self.criterion = nn.CrossEntropyLoss()
			self._plot_scatter = self._plot_boxplot_class

		else:
			print("Task " + self.task +"not recognized.\nOptions are \n\t 'reg': regression \n\t 'class': classifiation\n\n")
			sys.exit()

		# containers for the losses
		self.losses={'train': [],'valid': [], 'test':[]}


		# options for outputing the results
		self.tensorboard = tensorboard

		# output directory
		self.outdir = outdir
		if self.plot:
			if not os.path.isdir(self.outdir):
				os.mkdir(outdir)

		print('\n')
		print('='*40)
		print('=\t Convolution Neural Network')
		print('=\t model     : %s' %model_type)
		print('=\t CNN       : %s' %model.__name__)
		if self.data_set.select_feature == 'all':
			print('=\t features  : all')
		else:
			print('=\t features  : %s' %" / ".join([key for key,_ in self.data_set.select_feature.items()]))
		print('=\t targets   : %s' %self.data_set.select_target)
		print('=\t CUDA      : %s' %str(self.cuda))
		if self.cuda:
			print('=\t nGPU      : %d' %self.ngpu)
		print('='*40,'\n')	


		# check if CUDA works
		if self.cuda and not torch.cuda.is_available():
			print(' --> CUDA not deteceted : Make sure that CUDA is installed and that you are running on GPUs')
			print(' --> To turn CUDA of set cuda=False in DeepRankConvNet')
			print(' --> Aborting the experiment \n\n')
			sys.exit()


		# load the model
		self.net = model(data_set.input_shape)

		#multi-gpu
		if self.ngpu>1:
			ids = [i for i in range(self.ngpu)]
			self.net = nn.DataParallel(self.net,device_ids=ids).cuda()

		# cuda compatible
		elif self.cuda:
			self.net = self.net.cuda()

		# set the optimizer
		self.optimizer = optim.SGD(self.net.parameters(),lr=0.005,momentum=0.9,weight_decay=0.001)

	def train(self,nepoch=50, divide_set=[0.8,0.2], train_batch_size = 10, 
		      preshuffle = True,plot_intermediate=True,debug=False):

		'''
		Perform a simple training of the model. The data set is divided in training/validation sets

		ARGUMENTS

		nepoch : Int. number of iterations to go through the training 

		divide_set : the percentage assign to the training, validation and test set.

		train_batch_size : the mini batch size for the training
		
		preshuffle. Boolean Shuffle the data set before dividing it.

		plot_intermediate : plot scatter plots during the training

		'''

		if self.tensorboard:
			tbwriter = SummaryWriter('runs/'+datetime.now().strftime('%B%d  %H:%M:%S'))
		else:
			tbwriter = None

		# multi-gpu 
		if self.ngpu > 1:
			train_batch_size *= self.ngpu

		print('\n: Batch Size : %d' %train_batch_size)
		if self.cuda:
			print(': NGPU       : %d' %self.ngpu)

		# divide the set in train+ valid and test
		index_train,index_valid,index_test = self._divide_dataset(divide_set,preshuffle)


		# print the final; scatter plot
		self._plot_scatter(self.outdir+"/initial_prediction.png", indexes = [index_train,index_valid,index_test])
		
		# train the model
		self._train(index_train,index_valid,
			        index_test=index_test,
			        nepoch=nepoch,
			        train_batch_size=train_batch_size,
			        tensorboard_writer=tbwriter,
			        plot_intermediate=plot_intermediate,
			        debug=debug)

		# test the model
		self.test_loss = self._test(index_test,print_loss=True)

		# print the final; scatter plot

		self._plot_scatter(self.outdir+"/final_prediction.png", indexes = [index_train,index_valid,index_test])

		# close the writers	
		if self.tensorboard:
			tbwriter.close()



	def train_montecarlo(self,nmc=2,nepoch=50, divide_set=[0.8,0.1], train_batch_size = 10, preshuffle = True):

		'''
		Perform a monte carlo cross validation calculation

		ARGUMENTS

		nmc 		: Int The number of MC iterations

		nepoch : Int. number of iterations to go through the training

		divide_set : the percentage assign to the training, validation and test set. 

		train_batch_size : the mini batch size for the training
		
		preshuffle. Boolean Shuffle the data set before dividing it.
		'''

		if self.tensorboard:
			tbwriter = SummaryWriter('runs/'+datetime.now().strftime('%B%d  %H:%M:%S'))
		else:
			tbwrite = None

		# divide the indexes in train+valid and test
		index_train, index_valid, index_test = self._divide_dataset(divide_set,preshuffle)
		ntrain = len(index_train)

		# loop over the MC iterations
		lparam = []
		for imc in range(nmc):

			print("\n===============================================")
			print("=== %02d/%02d of Monte Carlo cross validation" %(imc+1,nmc))
			print("===============================================")

			# train the model
			lparam.append(self._train(index_train,index_valid,nepoch=nepoch,train_batch_size=train_batch_size,tensorboard_writer=tbwriter))
			
			# test the model
			self._test(index_test)

			# plot the final figure of merit
			self._plot_scatter(self.outdir+'/prediction_mc_%02d.png' %(imc+1), indexes = [index_train,index_valid,index_test])

			# shuffle the training / validation indexes
			index_train_valid = index_valid.tolist()+index_train.tolist()
			np.random.shuffle(index_train_valid)
			train_loader = data_utils.DataLoader(self.data_set,batch_size=train_batch_size,sampler=data_utils.sampler.SubsetRandomSampler(index_train_valid[:ntrain]))
			valid_loader = data_utils.DataLoader(self.data_set,batch_size=1,sampler=data_utils.sampler.SubsetRandomSampler(index_train_valid[ntrain:]))

			# reinit the parameters
			self._reinit_parameters()


		print("\n===============================================")
		print("===       Average of all the models" )
		print("===============================================")

		# apply the average parameters
		self._apply_average_parameters(lparam)

		# test and plot the average model
		self._test(index_test)
		self._plot_scatter(self.outdir+"/final_prediction.png",indexes = [index_train,index_valid,index_test])

		# close the writers	
		if self.tensorboard:
			tbwriter.close()

	def train_kfold(self,k=2,nepoch=50, divide_set=[0.8,0.1], train_batch_size = 10, preshuffle = True):

		'''
		Perform a kfold cross validation of the model. 
		The data set is divided in k folds in each fold k-1 folds are used as training
		and the remaining as validation.

		ARGUMENTS

		k 		: Int The number of fold

		nepoch : Int. number of iterations to go through the training

		divide_set : the percentage assign to the training, validation and test set. 

		train_batch_size : the mini batch size for the training
		
		preshuffle. Boolean Shuffle the data set before dividing it.
		'''

		if self.tensorboard:
			tbwriter = SummaryWriter('runs/'+datetime.now().strftime('%B%d  %H:%M:%S'))
		else:
			tbwrite = None


		# divide the indexes in train+valid and test
		index_train, index_valid, index_test = self._divide_dataset(divide_set,preshuffle)

		# get the possible conf of train and valid
		indexes_train, indexes_valid = self._kfold_index(k,index_train.tolist()+index_valid.tolist())

		# perform the k-fold cross validation
		lparam = []
		for ik,(index_train,index_valid) in enumerate(zip(indexes_train,indexes_valid)):

			print("\n===============================================")
			print("=== %02d of %02d-fold cross validation" %(ik+1,k))
			print("===============================================")

			# train the model
			lparam.append(self._train(index_train,index_valid,nepoch=nepoch,train_batch_size=train_batch_size,tensorboard_writer=tbwriter))

			# test the model
			self._test(index_test)

			# plot the final figure of merit
			self._plot_scatter(self.outdir+'/prediction_%dfoldcv_%d.png' %(k,ik+1), indexes = [index_train,index_valid,index_test])

			# reinit the parameters
			self._reinit_parameters()

		print("\n===============================================")
		print("=== Average all the models" )
		print("===============================================")

		# apply the average parameters
		self._apply_average_parameters(lparam)

		# test and plot the average model
		self._test(index_test)
		self._plot_scatter(self.outdir+"/final_prediction.png",indexes = [index_train,index_valid,index_test])

		# close the writers	
		if self.tensorboard:
			tbwriter.close()


	def save_model(self,epoch,filename='model.pth.tar'):
		'''
		save the model to disk
		'''
		state = {'epoch'       : epoch,
				'state_dict'   : self.net.state_dict(),
				'optimizer'    : self.optimizer.state_dict(),
				'losses'       : self.losses}
		torch.save(state,filename)

	def load_model(self,filename):
		'''
		load model
		'''
		state = torch.load(filename)
		self.net.load_state_dict(state['state_dict'])
		self.optimizer.load_state_dict(state['optimizer'])

		
	def _divide_dataset(self,divide_set, preshuffle):

		'''
		Divide the data set in atraining validation and test
		according to the percentage in divide_set
		Retun the indexes of  each set
		'''

		# get the indexes of the train/validation set
		ind = np.arange(self.data_set.__len__())
		ntot = len(ind)

		if self.data_set.test_database is None:

			print('No test data base found')
			print('Dividing the train dataset in:')
			print('  : 0.8 -> train')
			print('  : 0.1 -> valid')
			print('  : 0.1 -> test')

			divide_set = [0.8,0.1,0.1]

			if preshuffle:
				np.random.shuffle(ind)

			# size of the subset
			ntrain = int( float(ntot)*divide_set[0] )	
			nvalid = int( float(ntot)*divide_set[1] )
			ntest =  int( float(ntot)*divide_set[2] )
			

			# indexes
			index_train = ind[:ntrain]
			index_valid = ind[ntrain:ntrain+nvalid]
			index_test = ind[ntrain+nvalid:]

		else:

			if preshuffle:
				np.random.shuffle(ind[:self.data_set.ntrain])

			# size of the subset for training
			ntrain = int( float(self.data_set.ntrain)*divide_set[0] )	
			
			# indexes
			index_train = ind[:ntrain]
			index_valid = ind[ntrain:self.data_set.ntrain]
			index_test = ind[self.data_set.ntrain:]

		return index_train,index_valid,index_test


	def _kfold_index(self,k,index_train_valid):

		'''
		Return the indexes of the training and validation set
		in the frame of a k-fold cross validation training
		'''

		# get the indexes of the train/validation set
		ntot = len(index_train_valid)

		# size of the subset
		subsize = int( float(ntot)/k)

		# get the indexes
		fold_index = []
		for ik in range(k):
			fold_index.append(list(range(ik*subsize,(ik+1)*subsize)))

		indexes_train, indexes_valid = [],[]
		for ik in range(k):
			index_tmp= []
			for iik in range(k):
				if iik != ik:
					index_tmp += [index_train_valid[j] for j in fold_index[iik]]
					
			indexes_train.append(index_tmp)
			indexes_valid.append( [index_train_valid[j] for j in fold_index[ik]])
			
		return indexes_train,indexes_valid


	def _train(self,index_train,index_valid,
		       index_test=None,
		       nepoch = 50,train_batch_size = 5,
		       tensorboard_writer = None,plot_intermediate=False,debug=False):

		'''
		Train the model 
	
		Arguments
		
		index_train : the indexes of the training set
		index_valid : the indexes of the validation set
		nepoch : number of epochs to be performed
		train_batch_size : the mini batch size for the training
		tensorboard_write : the writer for tensor board
		plot_intermediate : plot itnermediate
		debug : if TRUE the data set are statically created
		'''

		# printing options
		nprint = int(nepoch/10)

		# store the length of the training set
		ntrain = len(index_train)

		# pin memory for cuda
		pin = False
		if self.cuda:
			pin = True


		if debug:

			# crete the valid/train data set
			dataset_train = DeepRankDataSet('')
			dataset_train.features =  self.data_set.features[:ntrain]
			dataset_train.targets = self.data_set.targets[:ntrain]
			dataset_train.input_shape = self.data_set.input_shape

			dataset_valid =  DeepRankDataSet('')
			dataset_valid.features =  self.data_set.features[ntrain:]
			dataset_valid.targets = self.data_set.targets[ntrain:]
			dataset_valid.input_shape = self.data_set.input_shape

			#  create the loaders
			train_loader = data_utils.DataLoader(dataset_train,batch_size=train_batch_size,shuffle=False)
			valid_loader = data_utils.DataLoader(dataset_valid,batch_size=1,shuffle=False)

		else:

			# create the sampler
			train_sampler = data_utils.sampler.SubsetRandomSampler(index_train)
			valid_sampler = data_utils.sampler.SubsetRandomSampler(index_valid)

			#  create the loaders
			train_loader = data_utils.DataLoader(self.data_set,batch_size=train_batch_size,sampler=train_sampler,pin_memory=pin)
			valid_loader = data_utils.DataLoader(self.data_set,batch_size=1,sampler=valid_sampler,pin_memory=pin)


		# training loop
		av_time = 0.0
		for epoch in range(nepoch):

			print('\n: epoch %03d / %03d ' %(epoch,nepoch) + '-'*45)
			t0 = time.time()
			# train the model
			self.train_loss = self._epoch(train_loader,train_model=True)
			self.losses['train'].append(self.train_loss)

			# validate the model
			self.valid_loss = self._epoch(valid_loader,train_model=False)
			self.losses['valid'].append(self.valid_loss)

			# test the model
			if index_test is not None:
				test_loss = self._test(index_test,print_loss=False)
				self.losses['test'] = test_loss

			# write the histogramm for tensorboard
			if self.tensorboard:
				self._export_tensorboard(tensorboard_writer,epoch)

			# talk a bit about losse
			print('  train loss       : %1.3e\n  valid loss       : %1.3e' %(self.train_loss, self.valid_loss))
			if index_test is not None:
				print('  test loss        : %1.3e' %(test_loss))

			# timer
			elapsed = time.time()-t0
			print('  epoch done in    :', time.strftime('%H:%M:%S', time.gmtime(elapsed)) )

			# remaining time
			av_time += elapsed
			nremain = nepoch-(epoch+1)
			remaining_time = av_time/(epoch+1)*nremain
			print('  remaining time   :',  time.strftime('%H:%M:%S', time.gmtime(remaining_time)))		

			# plot the scatter plots
			if self.plot:
				if plot_intermediate and epoch%nprint == nprint-1:
					self._plot_scatter(self.outdir+"/prediction_%03d.png" %epoch, loaders = [train_loader,valid_loader])


		return torch.cat([param.data.view(-1) for param in self.net.parameters()],0)


	def _test(self,index_test,print_loss=False):

		# test the model
		test_sampler = data_utils.sampler.SubsetRandomSampler(index_test)
		test_loader = data_utils.DataLoader(self.data_set,batch_size=1,sampler=test_sampler)
		test_loss = self._epoch(test_loader,train_model=False)
		if print_loss:
			print('\n-->\n-->\t\t Test loss : %1.3e\n-->' %test_loss)
		return test_loss

	def _epoch(self,data_loader,train_model=True):

		'''
		Perform one single epoch iteration over a data loader
		The option train is True or False and controls
		if the model should be trained or not on the data
		The loss of the model is returned
		'''

		running_loss = 0
		for (inputs,targets) in data_loader:

			# get the data
			inputs,targets = self._get_variables(inputs,targets)

			# zero gradient
			if train_model:
				self.optimizer.zero_grad()

			# forward + loss
			outputs = self.net(inputs)
			loss = self.criterion(outputs,targets)
			running_loss += loss.data[0]

			# backward + step
			if train_model:
				loss.backward()
				self.optimizer.step()

		return running_loss


	def _reinit_parameters(self):

		'''
		Reinitialize the parameters of the model with a normal distribution
		'''

		for name,param in self.net.named_parameters():
			nn.init.normal(param,mean=0,std=0.1)


	def _apply_average_parameters(self,param_list):

		'''
		Average the parameters contained in the param_list
		and apply the average to the model
		This is usefull at the end of multiple training
		to get the 'final' model and test its performance 
		'''

		avg_param = torch.zeros(param_list[0].size())
		nparam = len(param_list)		
		for param in param_list:
			avg_param += param/nparam

		offset = 0
		for param in self.net.parameters():
			param.data.copy_(avg_param[offset:offset+param.nelement()].view(param.size()))
			offset += param.nelement()

	def _get_variables(self,inputs,targets):

		'''
		Convert the inout/target in Variables
		the format is different for regression where the targets are float
		and classification where they are int.
		'''

		# if cuda is available
		if self.cuda:
			inputs = inputs.cuda(async=True)
			targets = targets.cuda(async=True)


		# get the varialbe as float by default
		inputs,targets = Variable(inputs),Variable(targets).float()

		# change the targets to long for classification
		if self.task == 'class':
			targets =  targets.long()

		return inputs,targets

	def _plot_scatter_reg(self,figname,loaders=None,indexes=None):

		'''
		Plot a scatter plots of predictions VS targets useful '
		to visualize the performance of the training algorithm 
		
		We can plot either from the loaders or from the indexes of the subset
		
		loaders should be either None or a list of loaders of maxsize 3
		loaders = [train_loader,valid_loader,test_loader]

		indexes should be a list of indexes list of maxsize 3
		indexes = [index_train,index_valid,index_test]
		'''

		# abort if we don't want to plot
		if self.plot == False:
			return


		print('\n --> Scatter Plot : ', figname, '\n')

		# check if we have loaders
		if loaders is None:

			# check if we have indexes
			if indexes is None:
				print('-> Error during scatter plot, you must provide either loaders or indexes')
				return 

			# create the loaders
			loaders = []
			for iind in range(len(indexes)):
				loaders.append(data_utils.DataLoader(self.data_set,sampler=data_utils.sampler.SubsetRandomSampler(indexes[iind])))
			
		color_plot = ['red','blue','green']
		labels = ['Train','Valid','Test']

		fig,ax = plt.subplots()	

		xvalues = np.array([])
		yvalues = np.array([])

		for idata,data_loader in enumerate(loaders):

			# storage for ploting
			plot_out,plot_targ = [],[]

			for (inputs,targets) in data_loader :

				inputs,targets = self._get_variables(inputs,targets)
				outputs = self.net(inputs)

				if self.cuda:
					plot_out +=  outputs.data.cpu().numpy().tolist()
					plot_targ += targets.data.cpu().numpy().tolist()
				else:
					plot_out +=  outputs.data.numpy().tolist()
					plot_targ += targets.data.numpy().tolist()
			

			plot_targ = np.array(plot_targ).flatten()
			plot_out = np.array(plot_out).flatten()

			plot_targ = self.data_set.backtransform_target(plot_targ)
			plot_out = self.data_set.backtransform_target(plot_out)

			xvalues = np.append(xvalues,plot_targ)
			yvalues = np.append(yvalues,plot_out)

			ax.scatter(plot_targ,plot_out,c = color_plot[idata],label=labels[idata])	

		
		legend = ax.legend(loc='upper left')
		ax.set_xlabel('Targets')
		ax.set_ylabel('Predictions')

		values = np.append(xvalues,yvalues)
		border = 0.1 * (values.max()-values.min())
		ax.plot([values.min()-border,values.max()+border],[values.min()-border,values.max()+border])

		fig.savefig(figname)
		plt.close()

	def _plot_scatter_class(self,figname,loaders=None,indexes=None):

		'''
		Plot a scatter plots of predictions VS targets useful '
		to visualize the performance of the training algorithm 
		This is only usefull in regression tasks
		
		We can plot either from the loaders or from the indexes of the subset
		
		loaders should be either None or a list of loaders of maxsize 3
		loaders = [train_loader,valid_loader,test_loader]

		indexes should be a list of indexes list of maxsize 3
		indexes = [index_train,index_valid,index_test]
		'''
		

		# abort if we don't want to plot
		if self.plot == False:
			return

		print('\n --> Scatter Plot : ', figname, '\n')

		# check if we have loaders
		if loaders is None:

			# check if we have indexes
			if indexes is None:
				print('-> Error during scatter plot, you must provide either loaders or indexes')
				return 

			# create the loaders
			loaders = []
			for iind in range(len(indexes)):
				loaders.append(data_utils.DataLoader(self.data_set,sampler=data_utils.sampler.SubsetRandomSampler(indexes[iind])))

		color_plot = ['red','blue','green']
		labels = ['Train','Valid','Test']

		verts = list(zip([-1., 1., 1., -1.], [-1., -1., 1., -1.]))
		marker = [(5,1),(verts,0),'o']
		area = [50,50,100]
		fig,ax = plt.subplots()	
		ax.plot([-1,1],[0,0],c='black')
		ax.plot([0,0],[-1,1],c='black')
		ax.set_xlim([-1.25,1.25])
		ax.set_ylim([-1.25,1.25])

		angles = np.linspace(0,2*np.pi,50)
		for r in [0.25,0.5,0.75,1]:
			ax.plot(r*np.cos(angles),r*np.sin(angles),'--',linewidth=0.5,c='black')

		for idata,data_loader in enumerate(loaders):

			# storage for ploting
			plot_out,plot_targ = [],[]
			ndata = data_loader.sampler.__len__()
			angles = np.linspace(0,2*np.pi,ndata)
			k = 0
			for (inputs,targets) in data_loader :

				inputs,targets = self._get_variables(inputs,targets)
				outputs = self.net(inputs)

				if self.cuda:
					tar = targets.data.cpu().numpy()
					out = outputs.data.cpu().numpy()
				else:
					tar = targets.data.numpy()
					out = outputs.data.numpy()
					
				for pts,t in zip(out,tar):
					col = color_plot[int(t)]
					angle = angles[k]
					r = F.softmax(torch.FloatTensor(pts)).data.numpy()
					ax.scatter(r[1]*np.cos(angle),r[1]*np.sin(angle),s=area[idata],c=col,alpha=0.5,marker=marker[idata])
					k+=1
		fig.savefig(figname)

	def _plot_boxplot_class(self,figname,loaders=None,indexes=None):

		'''
		Plot a boxplot of predictions VS targets useful '
		to visualize the performance of the training algorithm
		This is only usefull in classification tasks

		We can plot either from the loaders or from the indexes of the subset

		loaders should be either None or a list of loaders of maxsize 3
		loaders = [train_loader,valid_loader,test_loader]

		indexes should be a list of indexes list of maxsize 3
		indexes = [index_train,index_valid,index_test]
		'''

		# abort if we don't want to plot
		if self.plot == False:
			return

		# check if we have loaders
		if loaders is None:

			# check if we have indexes
			if indexes is None:
				print('-> Error during boxplot, you must provide either loaders or indexes')
				return

			# create the loaders
			loaders = []
			for iind in range(len(indexes)):
				loaders.append(data_utils.DataLoader(self.data_set,sampler=data_utils.sampler.SubsetRandomSampler(indexes[iind])))

		labels = ['Train','Valid','Test']

		fig, ax = plt.subplots(1, 3, sharey=True)

		data = [[], []]
		print("  Confusion matrices:")
		for idata,data_loader in enumerate(loaders):
			confusion=[[0, 0], [0, 0]]
			for (inputs,targets) in data_loader :

				inputs,targets = self._get_variables(inputs,targets)
				outputs = self.net(inputs)

				if self.cuda:
					tar = targets.data.cpu().numpy()
					out = outputs.data.cpu().numpy()
				else:
					tar = targets.data.numpy()
					out = outputs.data.numpy()

				for pts,t in zip(out,tar):
					r = F.softmax(torch.FloatTensor(pts)).data.numpy()
					data[t].append(r[1])
					confusion[t][r[1]>0.5] += 1
			print("  {:5s}: {:s}".format(labels[idata],str(confusion)))
			ax[idata].boxplot(data)
			ax[idata].set_xlabel(labels[idata])
			ax[idata].set_xticklabels(['0', '1'])
		fig.savefig(figname, bbox_inches='tight')

	def _export_tensorboard(self,tensorboard_writer,epoch):

		'''
		Export the data of the model to tensorboard for post visualisation
		'''

		for name,param in self.net.named_parameters():
			tensorboard_writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)

			# write the losses for tensorboard
			tensorboard_writer.add_scalar('train_loss',self.train_loss,epoch)
			tensorboard_writer.add_scalar('valid_loss',self.valid_loss,epoch)



#-------------------------------------------------------------------------------


if __name__ == "__main__":


	from deeprank.learn import DeepRankDataSet, DeepRankConvNet

	# create the sets
	data_folder = '../training_set/'
	data_set = DeepRankDataSet(data_folder,
                               filter_dataset = 'decoyID.dat',
                               select_feature={'AtomicDensities' : 'all'},
                               select_target='haddock_score')

	# create the network
	model = DeepRankConvNet(data_set,
                            SmallConvNet3D,
                            model_type='3d',
                            task='reg',
                            tensorboard=False,
                            outdir='./test_out/')

	# change the optimizer
	model.optimizer = optim.SGD(model.net.parameters(),
                                lr=0.001,
                                momentum=0.9,
                                weight_decay=0.005)

	# start the training
	model.train(nepoch = 250)

	






